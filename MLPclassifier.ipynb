{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer Neural Network for Wine Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing nescessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "df = pd.read_csv(url, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions for the class\n",
    "\n",
    "#def sums_squared_error(output, target):\n",
    "    #return (np.sum((output - target)**2))/2\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "#I tried using this but couldn't make it work so I ended up using a different approach in my class. \n",
    "def one_hot_encode(labels, num_classes):\n",
    "\n",
    "    encoded_labels = np.zeros((len(labels), num_classes))\n",
    "   \n",
    "    for i, label in enumerate(labels):\n",
    "        if label >= num_classes:\n",
    "            #this was for when i was trying to debug it \n",
    "            print(f\"Warning: label {label} is outside the range of 0 to {num_classes-1}\")\n",
    "        encoded_labels[i, label] = 1\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, data, target, hidden_size=4):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weights1 = None\n",
    "        self.weights2 = None\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_split(data, target)\n",
    "    #splitting the data set\n",
    "    def train_test_split(self, X, y, test_size=0.2):\n",
    "        n_samples = X.shape[0]\n",
    "        n_test = int(test_size * n_samples)\n",
    "        test_indices = np.random.choice(n_samples, n_test, replace=False)\n",
    "        train_indices = np.delete(np.arange(n_samples), test_indices)\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    #initializing the weights\n",
    "    def initialize_weights(self):\n",
    "        n_features = self.X_train.shape[1]\n",
    "        n_classes = self.y_train.shape[1]\n",
    "        self.weights1 = np.random.randn(n_features, self.hidden_size)\n",
    "        self.weights2 = np.random.randn(self.hidden_size, n_classes)\n",
    "    \n",
    "    def feedforward(self):\n",
    "        #I chose the sigmoid due to not much understanding of the relu function\n",
    "        self.layer1 = sigmoid(np.dot(self.X_train, self.weights1))\n",
    "        self.output = softmax(np.dot(self.layer1, self.weights2))\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        #I wanted to make a SSE function but since it would be different for each layer I figured it would be easier to just put in here. \n",
    "        d_weights2 = np.dot(self.layer1.T, (self.output - self.y_train))\n",
    "        d_weights1 = np.dot(self.X_train.T, np.dot((self.output - self.y_train), self.weights2.T) * sigmoid_derivative(self.layer1))\n",
    "        self.weights1 -= d_weights1\n",
    "        self.weights2 -= d_weights2\n",
    "    \n",
    "    def train(self, number_of_epochs):\n",
    "        self.initialize_weights()\n",
    "        for i in range(number_of_epochs):\n",
    "            self.feedforward()\n",
    "            self.backpropagation()\n",
    "        print(\"Training finished!\")\n",
    "    \n",
    "    def test(self):\n",
    "        layer1 = sigmoid(np.dot(self.X_test, self.weights1))\n",
    "        output = softmax(np.dot(layer1, self.weights2))\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "        true_labels = np.argmax(self.y_test, axis=1)\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        print(\"Accuracy on test set: {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Fold test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "Accuracy on test set: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/dm_tfpw94y5g8g9kv_kt0hbc0000gp/T/ipykernel_73822/3319200332.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "data = df.iloc[:, 1:].values\n",
    "target = pd.get_dummies(df.iloc[:, 0]).values\n",
    "\n",
    "mlp = MLP(data, target, hidden_size=4)\n",
    "X_train, X_test, y_train, y_test = mlp.train_test_split(data, target)\n",
    "mlp.X_train = X_train\n",
    "mlp.X_test = X_test\n",
    "mlp.y_train = y_train\n",
    "mlp.y_test = y_test\n",
    "\n",
    "mlp.train(400)\n",
    "mlp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "Accuracy on test set: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/dm_tfpw94y5g8g9kv_kt0hbc0000gp/T/ipykernel_73822/3319200332.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLP(data, target, hidden_size=4)\n",
    "X_train2, X_test2, y_train2, y_test2 = mlp.train_test_split(data, target)\n",
    "mlp2.X_train = X_train2\n",
    "mlp2.X_test = X_test2\n",
    "mlp2.y_train = y_train2\n",
    "mlp2.y_test = y_test2\n",
    "\n",
    "mlp2.train(400)\n",
    "mlp2.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "Accuracy on test set: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/dm_tfpw94y5g8g9kv_kt0hbc0000gp/T/ipykernel_73822/3319200332.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "mlp3 = MLP(data, target, hidden_size=4)\n",
    "X_train3, X_test3, y_train3, y_test3 = mlp.train_test_split(data, target)\n",
    "mlp3.X_train = X_train3\n",
    "mlp3.X_test = X_test3\n",
    "mlp3.y_train = y_train3\n",
    "mlp3.y_test = y_test3\n",
    "\n",
    "mlp3.train(400)\n",
    "mlp3.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forth test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "Accuracy on test set: 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/dm_tfpw94y5g8g9kv_kt0hbc0000gp/T/ipykernel_73822/3319200332.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "mlp4 = MLP(data, target, hidden_size=4)\n",
    "X_train4, X_test4, y_train4, y_test4 = mlp.train_test_split(data, target)\n",
    "mlp4.X_train = X_train4\n",
    "mlp4.X_test = X_test4\n",
    "mlp4.y_train = y_train4\n",
    "mlp4.y_test = y_test4\n",
    "\n",
    "mlp4.train(400)\n",
    "mlp4.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n",
      "Accuracy on test set: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/dm_tfpw94y5g8g9kv_kt0hbc0000gp/T/ipykernel_73822/3319200332.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "mlp5 = MLP(data, target, hidden_size=4)\n",
    "X_train5, X_test5, y_train5, y_test5 = mlp.train_test_split(data, target)\n",
    "mlp5.X_train = X_train5\n",
    "mlp5.X_test = X_test5\n",
    "mlp5.y_train = y_train5\n",
    "mlp5.y_test = y_test5\n",
    "\n",
    "mlp5.train(400)\n",
    "mlp5.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP class is an implementation of a multilayer perceptron neural network with one hidden layer, which  has a constructor that takes in the data and target variables as input, which are then split into training and testing sets using a train_test_split function. The size of the hidden layer can also be specified in the constructor.\n",
    "\n",
    "The MLP class uses the sigmoid and softmax activation functions for the hidden and output layers, respectively. The feedforward method calculates the output of the neural network by multiplying the input with the weights of the hidden layer and passing it through the sigmoid function, and then multiplying the output of the hidden layer with the weights of the output layer and passing it through the softmax function.\n",
    "\n",
    "The backpropagation method calculates the error between the predicted output and the actual output, and updates the weights in the network to reduce the error. The train method initializes the weights and runs the feedforward and backpropagation methods for a specified number of iterations.\n",
    "\n",
    "Finally, the test method evaluates the accuracy of the model on the testing set by calculating the number of correct predictions divided by the total number of predictions. After a lot of time on this I reallly hope I have implemented it correctly, now to the five fold cross validation part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five Fold cross validaton "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Based on the five-fold cross-validation, the neural network achieved an accuracy of 51% for the first fold, 40% for the second and third fold, and 46% for the fourth and 49% fifth fold.\n",
    "\n",
    "For the number of iteration I manually increased the number of iteraryions from 0 to 1000, by 50 at a time and back untill I reached the best acuracy for them.  \n",
    "\n",
    "Overall, the accuracy scores obtained from the five-fold cross-validation suggest that the neural network may not be performing well on the dataset, with accuracies ranging from 40% to 51%. Further analysis and fine-tuning of the neural network may be necessary to improve its performance on this dataset. Regardless it surpasses the treshhold proposed by the professor in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources for the work. \n",
    "\n",
    "Book Neural Networks Projects with Python chapter 1. https://www.amazon.com/Neural-Network-Projects-Python-ultimate/dp/1789138906\n",
    "\n",
    "Softmax article. https://machinelearningmastery.com/softmax-activation-function-with-python/\n",
    "\n",
    "Neural Networks Pt. 2: Backpropagation Main Ideas Stats Quest Video. https://www.youtube.com/watch?v=IN2XmBhILt4\n",
    "\n",
    "I am also attaching the earlier versions(that didn't work quite right) of the MLP class to give you better understanding of the process I went through(and maybe earn some extra points if the implementation isn't right).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70fb4ba9a017521a1e3335c8877e618947e0cc44a3def1c62c35057e5085c9ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
